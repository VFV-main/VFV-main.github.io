---
layout: post
title: 'Loss Function'
categories: [ML,DL]
tags: Loss function
---

손실 함수 (Loss Func) 

 L(x, y ; $$$\theta$$$)  
 
 x : 입력
 y : 정답
 
 알고리즘 학습과정에 큰 기여를 함
 
 손실함수가 최소인 $$$\tilde{\theta}$$$를 찾아야 함
 
 $$$\tilde{\theta} = argmin_\theta L(x,y;\theta)$$$
.



 MSE (Mean Squared Error)
 
 $$$ E= \frac{1}{2}∑_i(y_i-\hat y_i)^2 $$$
 
     오차가 커질수록 손실함수가 빠르게 증가

 MAE (Mean Absolute Error)
$$$ E= ∑_i|y_i-\hat y_i| $$$
 
     오차가 커져도 손실함수가 일정하게 증가하기때문에 이상치에 면역이 있음

  CEE (Cross Entropy Error)  : (Softmax + One_hot_enc)
  
  $$$ E= -∑_iy_ilog\,\hat y_i $$$
     오차를 내는 과정에 정답 클래스만 비교하지만 Softmax함수로 인해 다른 클래스에도 영향을 미침

* * *

활성함수 (Activation Func)

Tanh ( Hyperbolic-tangent )

    Decisi boundary 간격에 대해 유연하게 사고하는 Soft decision

$$$f\left(x\right) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$$

![](https://paperswithcode.com/media/methods/Screen_Shot_2020-05-27_at_4.23.22_PM_dcuMBJl.png)


Sigmoid

    0 부터 시작해서 1까지의 범위 값을 가지며 확률로 표현이 가능   (이진 분류에 좋음)

$$${\displaystyle S(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{e^{x}+1}}=1-S(-x).}$$$

![](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)


Soft-max

    각 입력의 지수함수를 정규화 한것      (모든 출력의 합이 1 이 됨)   ( 다중 분류 )

![](https://dthumb-phinf.pstatic.net/?src=%22http%3A%2F%2Fufldl.stanford.edu%2Fwiki%2Fimages%2Fthumb%2F5%2F5c%2FStacked_Combined.png%2F500px-Stacked_Combined.png%22&type=w2)


![](https://mblogthumb-phinf.pstatic.net/MjAxNzAzMDNfMjk1/MDAxNDg4NTIwNjY5MDM4.WD5xYW0HXTM_Y0wNX2-37WRTcbvUAcaN8m5W2ZnnRqkg.q0fv8J2OOV_Up9VVLsRg6L5gw0X7eiQ9nV4Do489Hrcg.PNG.sogangori/softmax.png?type=w2)


$$$ f\left(x_i\right) = \frac{e^{x_i}}{∑_je^{x_j}} $$$

![](https://www.researchgate.net/profile/Muhammad-Imran-Babar/publication/332513298/figure/fig7/AS:753613810974724@1556686942863/Graph-of-Softmax-Function-Results.ppm)


Softmax + Sigmoid

$$$Softmax([x,0])_0 = \frac{e^x}{e^x+e^0} = \frac{1}{1+e^-x}$$$

    Sigmoid 는 하나의 입력을 0 으로 강제한다면 2-class Softmax와 동일함
      2 가지 클래스 구분에 1개의 입력을 받음



ReLU (Rectified Linear Unit)

$$$ ReLU\left (x\right) = max(0,x)$$$


    0 보다작은것은 0 으로 강제함
    미분값이 일정하며 연산이 빠르다는 장점이 있어  많이 쓰임




사진 한 장으로 정리가 된다.

![](https://qph.fs.quoracdn.net/main-qimg-07bc0ec05532caf5ebe8b4c82d0f5ca3)


_ _ _

Optimization